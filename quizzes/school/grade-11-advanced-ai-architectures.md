# Advanced AI Architectures and Transformers

**Category**: school
**Level**: advanced
**Topics**: transformers, attention mechanisms, BERT, GPT, advanced architectures
**Questions**: 11

---

## About This Quiz

Eleventh graders dive into modern AI architectures, focusing on Transformers and attention mechanisms. These concepts power state-of-the-art language models and computer vision systems.

---

## Questions

### Question 1
**Type**: multiple-choice
**Difficulty**: hard

**What is the transformer architecture?**

- A) A robot that changes shape
- B) A neural network architecture based on attention mechanisms, introduced in 2017
- C) An electrical device
- D) A method for changing data format

---

### Question 2
**Type**: true-false
**Difficulty**: hard

**Attention mechanisms allow a model to focus on relevant parts of input.**

---

### Question 3
**Type**: multiple-choice
**Difficulty**: hard

**What is self-attention?**

- A) Paying attention to yourself
- B) A mechanism where each element attends to other elements in the same sequence
- C) Monitoring your own performance
- D) A type of meditation

---

### Question 4
**Type**: true-false
**Difficulty**: hard

**BERT is a pre-trained language model based on the Transformer architecture.**

---

### Question 5
**Type**: multiple-choice
**Difficulty**: hard

**What does "pre-training" mean in the context of modern language models?**

- A) Training before you start
- B) Training a model on large unlabeled data for general language understanding before fine-tuning for specific tasks
- C) Training in advance of an event
- D) Initial training only

---

### Question 6
**Type**: true-false
**Difficulty**: hard

**GPT models use Transformer decoders and are designed for text generation.**

---

### Question 7
**Type**: multiple-choice
**Difficulty**: hard

**What is the difference between BERT and GPT?**

- A) One is better than the other
- B) BERT is bidirectional (sees left and right context); GPT is unidirectional (left-to-right)
- C) BERT is older
- D) They're the same thing with different names

---

### Question 8
**Type**: true-false
**Difficulty**: hard

**Vision Transformers (ViTs) apply the Transformer architecture to image recognition tasks.**

---

### Question 9
**Type**: multiple-choice
**Difficulty**: hard

**What is an embedding in modern language models?**

- A) Code embedded in a webpage
- B) A numerical vector representation that captures semantic meaning of tokens/words
- C) Text embedded in an image
- D) A hidden layer in a neural network

---

### Question 10
**Type**: true-false
**Difficulty**: hard

**The attention mechanism in Transformers uses query, key, and value vectors.**

---

### Question 11
**Type**: multiple-choice
**Difficulty**: hard

**What is prompt engineering?**

- A) Engineering that requires prompts
- B) The art and science of crafting input text to guide large language models to produce desired outputs
- C) Engineering a computer prompt
- D) Prompting engineers

\newpage

## Answer Key

| Question | Answer | Difficulty |
|----------|--------|------------|
| 1        | B      | hard       |
| 2        | True   | hard       |
| 3        | B      | hard       |
| 4        | True   | hard       |
| 5        | B      | hard       |
| 6        | True   | hard       |
| 7        | B      | hard       |
| 8        | True   | hard       |
| 9        | B      | hard       |
| 10       | True   | hard       |
| 11       | B      | hard       |

---

## Scoring Guide

- **100%**: Perfect! You master advanced AI!
- **91-99%**: Excellent! You understand Transformers!
- **82-90%**: Very good! Strong architecture knowledge!
- **73-81%**: Good effort! Advanced topics are complex!
- **Below 73%**: Keep learning! These concepts are cutting-edge!

---

## Resources

- Original "Attention Is All You Need" paper
- BERT and GPT research papers
- Vision Transformer papers
- Hugging Face model tutorials
